[TOC]

# SGD

容易陷入局部最优和随着当前梯度震荡&#x20;

# SGD-M&#x20;

基于历史梯度引入一阶动量，加速收敛（历史梯度累积），减少震荡（削弱了当前梯度的影响）&#x20;

# NAG&#x20;

先根据累积梯度向前走一步，求解未来一步的梯度，然后按照SGD-M的方式进行梯度下降，可以一定程度的跳出局部最优&#x20;

# Adagrad&#x20;

基于历史梯度引入二阶动量，自适应学习率，频繁更新的参数学习率会逐渐变小，反之，很少更新的参数学习率会相对大一些；很适合特征稀疏的场景。&#x20;

# Adadelta/RMSProp&#x20;

解决Adagrad二阶动量因历史梯度平方和的累加导致收敛过快，导致训练提前结束；改成历史累加变为过去一段时间内梯度平方和的累加，避免持续累加导致提前收敛的问题。（但由于是过去一段时间的，可能会导致学习率震荡，尤其当数据分布产生较大变化的时候，Adam也有这个问题）&#x20;

# Adam&#x20;

综合SGD-M（Momentum）和Adadelta,同时引入一阶动量和二阶动量，集大成者&#x20;

# NAdam&#x20;

综合Adam和NAG&#x20;

# 补充&#x20;

Adam通常还不错，也有使用Adagrad(学习率越来越小，稳定收敛），还有使用Adam结合SGD的（先使用Adam加速收敛，再使用SGD探索最优解），具体还是要结合自身数据特性选择优化算法，通常先用小数据集验证，如果数据稀疏，可以优先考虑自适应学习率的算法，可以通过Adam之类的快速调参，上线前用SGD精调。
